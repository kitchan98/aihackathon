{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LExaPuw6oGeZ",
        "outputId": "a08d8db1-b04a-43a0-b179-fec4efac91da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "with open('complete_tex.tex','r') as f:\n",
        "  latex_string = f.readlines()\n",
        "\n",
        "figure_list = []\n",
        "Section_headings = {}\n",
        "for count,lines in enumerate(latex_string):\n",
        "  # print(lines)\n",
        "  if re.match(r'\\\\section', lines):\n",
        "    head = lines.lstrip(r'\\\\section{').rstrip().rstrip('}')\n",
        "    Section_headings[head] = 0\n",
        "  if re.search(r'\\\\includegraphics', lines):\n",
        "    try:\n",
        "      figure_list.append(lines.split(']{')[1].rstrip().rstrip('}'))\n",
        "    except:\n",
        "      pass\n",
        "print(figure_list)\n",
        "print(Section_headings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_yheF7P4CDQ",
        "outputId": "0c741a37-a399-4bb1-b557-dfcaa576587a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Figures/pipeline.png', 'Figures/param_sen.png']\n",
            "{'Introduction': 0, 'Method': 0, 'Experiments': 0, 'Conclusion': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prompt_builder(title,page_length,count):\n",
        "    return f'''\n",
        "    {{Section Name:  {title},\n",
        "    Slide Information: {{\n",
        "      Number of Slides: {page_length},\n",
        "      Speaker Notes: {{\"Slide {{Number}}\": 150 words speech for slide}},\n",
        "      Table: Latex Table to header and row dictionary if present in {title},\n",
        "      Image: Figure path if it is present in {title} section (possible selections {figure_list}),\n",
        "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
        "    }}\n",
        "    }}\n",
        "    '''\n",
        "\n",
        "user_input = input('Enter the page length for each section: ')\n",
        "user_input = user_input.split(',')\n",
        "print(user_input)\n",
        "\n",
        "for count,key in enumerate(Section_headings):\n",
        "  Section_headings[key] = user_input[count]\n",
        "\n",
        "prompt_base = ''\n",
        "for count,(key,val) in enumerate(Section_headings.items()):\n",
        "  prompt_base+=prompt_builder(key,val,count+1)\n",
        "\n",
        "print(prompt_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKfBJ1woZ5e1",
        "outputId": "f690b690-62ea-4442-a667-612479df5390"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the page length for each section: 3,2,1,1\n",
            "['3', '2', '1', '1']\n",
            "\n",
            "    {Section Name:  Introduction,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 3,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Introduction,\n",
            "      Image: Figure path if it is present in Introduction section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n",
            "    {Section Name:  Method,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 2,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Method,\n",
            "      Image: Figure path if it is present in Method section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n",
            "    {Section Name:  Experiments,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 1,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Experiments,\n",
            "      Image: Figure path if it is present in Experiments section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n",
            "    {Section Name:  Conclusion,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 1,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Conclusion,\n",
            "      Image: Figure path if it is present in Conclusion section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are an expert slide expert that extracts latex papers and create presentation slides.\",\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": f'''\n",
        "      Given the document: {latex_string}\n",
        "      Analyse paper by section and strictly follow this template but json format:\n",
        "      {prompt_base}\n",
        "      '''\n",
        "    }\n",
        "  ]\n",
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYXc0graMUk",
        "outputId": "0a91340e-3e54-4b95-d286-43013e9ef01c"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are an expert slide expert that extracts latex papers and create presentation slides.'}, {'role': 'user', 'content': '\\n      Given the document: [\\'\\\\\\\\documentclass[sigconf,nonacm]{acmart}\\\\n\\', \\'\\\\\\\\makeatletter\\\\n\\', \\'\\\\\\\\renewcommand\\\\\\\\@formatdoi[1]{\\\\\\\\ignorespaces}\\\\n\\', \\'\\\\\\\\makeatother\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\AtBeginDocument{\\\\\\\\providecommand\\\\\\\\BibTeX{{\\\\\\\\normalfont B\\\\\\\\kern-0.5em{\\\\\\\\scshape i\\\\\\\\kern-0.25em b}\\\\\\\\kern-0.8em\\\\\\\\TeX}}}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\acmDOI{}\\\\n\\', \\'\\\\\\\\acmISBN{}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\usepackage{hyperref}\\\\n\\', \\'\\\\\\\\usepackage{siunitx}\\\\n\\', \\'\\\\\\\\usepackage{xspace}\\\\n\\', \\'\\\\\\\\usepackage{multirow}\\\\n\\', \\'\\\\\\\\usepackage{enumitem}\\\\n\\', \\'\\\\\\\\usepackage{placeins}\\\\n\\', \\'\\\\\\\\usepackage{natbib}\\\\n\\', \\'\\\\\\\\newcommand{\\\\\\\\ours}{\\\\\\\\textsc{TinyLLM}\\\\\\\\xspace}\\\\n\\', \\'\\\\\\\\settopmatter{printacmref=false}\\\\n\\', \\'\\\\\\\\setcopyright{none}\\\\n\\', \\'\\\\\\\\pagestyle{plain}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{document}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\title{\\\\\\\\ours: Learning a Small Student from Multiple Large Language Models}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Yijun Tian}\\\\n\\', \\'\\\\\\\\authornote{Equally contributed.}\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of Notre Dame}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{yijun.tian@nd.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Yikun Han}\\\\n\\', \\'\\\\\\\\authornotemark[1]\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of Michigan}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{yikunhan@umich.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Xiusi Chen}\\\\n\\', \\'\\\\\\\\authornotemark[1]\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of California, Los Angeles}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{xchen@cs.ucla.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Wei Wang}\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of California, Los Angeles}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{weiwang@cs.ucla.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Nitesh V. Chawla}\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of Notre Dame}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{nchawla@nd.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{abstract}\\\\n\\', \\'  Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose \\\\\\\\ours, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that \\\\\\\\ours can outperform large teacher LLMs significantly, despite having a considerably smaller model size.\\\\n\\', \\'\\\\\\\\end{abstract}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\maketitle\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\section{Introduction}\\\\n\\', \\'Large language models (LLMs) have recently taken over various domains and applications, including society ~\\\\\\\\cite{rao2023makes}, education~\\\\\\\\cite{zelikman2023generating}, and scientific understanding~\\\\\\\\cite{beltagy2019scibert}. Despite the success of larger emerging models (GPT-4, Claude-2), their smaller counterparts hardly demonstrate such promising capabilities for performing complex reasoning~\\\\\\\\cite{wei2022emergentabilities,chen2023gotta}. This has been unveiled as the well-known scaling law of LLMs~\\\\\\\\cite{kaplan2020scalinglaws}. As such, it has been desirable to transfer the capabilities of the larger models to the smaller ones so that the smaller ones could be easily deployed while still enjoying the strong capabilities. Previous studies have shown that knowledge distillation is an instrumental tool in mitigating the performance gap between larger models such as LLMs and smaller ones~\\\\\\\\cite{wan2023efficient, hsieh-etal-2023-distilling, kd_survey}. Examples of effective distillation methods include DistilBERT~\\\\\\\\cite{sanh2019distilbert}, Alpaca~\\\\\\\\cite{alpaca} and Vicuna~\\\\\\\\cite{zheng2023judging}.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'However, existing methods suffer from two major drawbacks: (1) \\\\\\\\textbf{Limited Knowledge Diversity}: Current research predominantly employs a single-teacher approach, which confines the learning scope of the student model to the knowledge derived from its own training and architecture designs \\\\\\\\cite{ho2022largelanguagemodels,magister2022teachingsmall,li2023symbolicchain,wang2022pinto}. This means that the student model is limited to the perspectives, biases, and potential weaknesses of the teacher. (2) \\\\\\\\textbf{Lack of Rich Contextual Information}: While rationales play a vital role in effective reasoning \\\\\\\\cite{wei2022chainofthought,kojima2022zero}, current research primarily focuses on leveraging ground truth labels, which indicate the correct answer but do not provide insights into the reasoning and thought process behind that answer.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'In response to the above issues, we propose \\\\\\\\ours, a paradigm that improves the reasoning ability of small student LLMs by distilling knowledge from multiple large teacher LLMs with rationale guidance. Specifically, \\\\\\\\ours mitigates the limited knowledge diversity issue by involving multiple teacher models as \\\\\\\\textit{co-advisors}. To fully exploit each teacher model and mitigate the lack of rich contextual information problem, \\\\\\\\ours asks the teacher for the rationales to support the answers. By learning from multiple teachers, the student model can inherit a broader range of skills and knowledge, leading to better generalization capabilities. In addition, to ensure the rationales are grounded in contextually appropriate scenarios and reflect the true underlying reasoning procedure, \\\\\\\\ours features an in-context example generator and a teacher-forcing Chain-of-Thought strategy, making the teachers understand the task through demonstrations and generate the accurate rationales.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'To thoroughly assess our approach, we conduct experiments on six datasets in commonsense and biomedical reasoning tasks. The results show that the usage of our paradigm enhances performance by \\\\\\\\textbf{+5.07\\\\\\\\%} to \\\\\\\\textbf{+12.57\\\\\\\\%} compared to full fine-tuning with significantly smaller model size, i.e., \\\\\\\\textbf{1.1\\\\\\\\%} to \\\\\\\\textbf{26.0\\\\\\\\%} of teacher sizes. We also perform ablation studies to demonstrate the validity of teacher rationales and undertake hyperparameter analyses for a comprehensive evaluation. To summarize, our main contributions are as follows:\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{figure*}[ht]\\\\n\\', \\'\\\\\\\\begin{center}\\\\n\\', \\'\\\\\\\\includegraphics[width=\\\\\\\\textwidth]{Figures/pipeline.png}\\\\n\\', \\'\\\\\\\\end{center}\\\\n\\', \\'\\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\caption{\\\\n\\', \\'Pipeline of \\\\\\\\ours: Given an input question, we first generate in-context examples and obtain rationales from multiple large LLMs via a teacher-forcing Chain-of-Thought Strategy. Later, a small student LLM is trained to integrate rationales from different teachers via multi-task instruction tuning, along with the ground truth label.\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\label{fig:pipeline}\\\\n\\', \\'\\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\end{figure*}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{itemize}[nosep,leftmargin=*]\\\\n\\', \\'\\\\\\\\item We identify two problems with existing work on learning smaller language models: 1) limited knowledge diversity and 2) lack of rich contextual information.\\\\n\\', \\'\\\\\\\\item To address the problems, we propose \\\\\\\\ours, a novel knowledge distillation paradigm to learn a small student LLM by transferring reasoning capabilities from multiple large teacher LLMs. We encourage the student LLM to understand the rationale behind the generated answer.\\\\n\\', \\'\\\\\\\\item Extensive experiments validate the superiority of \\\\\\\\ours across six datasets and two reasoning tasks. \\\\n\\', \\'\\\\\\\\ours can achieve up to \\\\\\\\textbf{+12.57\\\\\\\\%} of performance improvement with 1.1\\\\\\\\% of model size.\\\\n\\', \\'\\\\\\\\end{itemize}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\section{Method}\\\\n\\', \\'The pipeline of \\\\\\\\ours is shown in Figure \\\\\\\\ref{fig:pipeline}.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\subsection{Preliminary}\\\\n\\', \\'\\\\\\\\textbf{Multiple Choice Question Answering.} A $k$-way multiple choice question answering (MCQA) is defined as follows: Given a question $Q_i$, a set of candidate answer options $O_i=\\\\\\\\{O_{i1},O_{i2},...,O_{ik}\\\\\\\\}$, the model is tasked with selecting the correct answer from the set $O_i$, such that the selected answer aligns the ground truth label $A_i$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Knowledge Distillation.} \\\\n\\', \\'The knowledge distillation process begins with the teacher model, denoted as $T$ parameterized by $\\\\\\\\theta_T$, which has been pre-trained on a large corpus. Later, the student model, $S$, with parameter $\\\\\\\\theta_S$, is tasked with distilling knowledge directly from $T$, leveraging the strong capabilities of $T$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Obtaining Rationales from Teachers}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent\\\\n\\', \\'\\\\\\\\textbf{In-context Example Generator.}\\\\n\\', \\'To enable the rationales that are generated by teachers to be grounded in contextually appropriate scenarios, we introduce an optional in-context example generator. This tool is designed to produce in-context examples for any given input, providing more detailed information about the input data and task. For simplicity, we select the examples randomly within the same dataset. This aids the teacher LLMs in comprehending the nature and specifics of the task more deeply. By integrating this generator, we facilitate a more informed and nuanced generation of rationales by the teacher models, enhancing the learning experience for the student model. \\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent\\\\n\\', \\'\\\\\\\\textbf{Teacher-forcing Chain-of-Thought.}\\\\n\\', \\'In addition, we design a teacher-forcing strategy to ensure the validity of the rationales. Compared to existing methods that simply employ regular chain-of-thought (CoT) mechanisms \\\\\\\\cite{wei2022chainofthought,kojima2022zero}, wherein an LLM is prompted with sets of questions and options \\\\\\\\(\\\\\\\\{Q_i, O_i\\\\\\\\}\\\\\\\\) to elicit rationales \\\\\\\\(R_i\\\\\\\\) directly, \\\\\\\\ours posits a distinct advantage in integrating the correct answer \\\\\\\\(A_i\\\\\\\\) into the input. We hypothesize that the inclusion of \\\\\\\\(A_i\\\\\\\\) alongside \\\\\\\\(Q_i\\\\\\\\) and \\\\\\\\(O_i\\\\\\\\) facilitates a more nuanced understanding of the input context and the correct logical rationales leading to the answer, thereby facilitating a more informed and accurate generation process. Specifically, we consider the concatenation of questions, options, and answers \\\\\\\\(\\\\\\\\{Q_i, O_i, A_i\\\\\\\\}\\\\\\\\) as the input to LLMs.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent\\\\n\\', \\'\\\\\\\\textbf{Rationales from Multiple Teachers.}\\\\n\\', \\'Given $M$ teachers, \\\\\\\\ours pioneers the usage of a multi-teacher architecture in which each teacher \\\\\\\\(T^m\\\\\\\\) is an LLM. In particular, the rationale \\\\\\\\(R_i^m\\\\\\\\) produced by a specific teacher model $\\\\\\\\theta_{T^m}$ for the \\\\\\\\(i\\\\\\\\)th question is derived using the question $Q_i$, options $O_i$, correct answer $A_i$, and in-context examples $P_i$. The process is formalized as follows:\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'R_i^m = T^m(Q_i, O_i, A_i, P_i; \\\\\\\\theta_{T^m}).\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{table*}[ht]\\\\n\\', \\'\\\\\\\\caption{Overall experimental results. The best results across different LLM sizes are highlighted in bold. $\\\\\\\\Delta_{FF}$ represents the relative performance improvement of \\\\\\\\ours to Full Fine-Tuning. Accuracy is used as the evaluation metric.}\\\\n\\', \\'\\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\label{Overall}\\\\n\\', \\'  \\\\\\\\begin{tabular}{ccccccccc}\\\\n\\', \\'    \\\\\\\\toprule\\\\n\\', \\'     &  & \\\\\\\\multicolumn{4}{c}{Commonsense Reasoning} & \\\\\\\\multicolumn{2}{c}{Biomedical Reasoning} & \\\\\\\\\\\\\\\\ \\\\n\\', \\'    \\\\\\\\cmidrule{3-8}\\\\n\\', \\'    \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{LLM}}  & \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{Method}} & {\\\\\\\\textbf{OBQA}} & {\\\\\\\\textbf{ARC}} & {\\\\\\\\textbf{PIQA}} & {\\\\\\\\textbf{Riddle}} & {\\\\\\\\textbf{PQA}} & {\\\\\\\\textbf{BioASQ}} & \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{Total}}\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    \\\\\\\\multirow{2}{*}{3B/7B Teacher} & FLAN-T5 xlarge & 69.20 & 68.24 & 58.43 & 53.73 & 71.50 & 65.85 & 64.49\\\\\\\\\\\\\\\\\\\\n\\', \\'    & LLaMA 2 & 58.60 & 45.90 & 78.80 & 47.65 & 54.50 & 73.75 & 59.87\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    \\\\n\\', \\'    \\\\\\\\multirow{5}{*}{\\\\\\\\begin{tabular}[c]{@{}c@{}}80M Student\\\\\\\\\\\\\\\\Size: 2.7\\\\\\\\%/1.1\\\\\\\\%\\\\\\\\end{tabular}} & Inference & 16.60 & 19.31 & 20.78 & 13.33 & 38.00 & 47.97 & 26.00 \\\\\\\\\\\\\\\\\\\\n\\', \\'     &  LoRA & 37.80 & 27.12 & 39.93 & 39.80 & 53.75 & 78.05 & 46.08 \\\\\\\\\\\\\\\\\\\\n\\', \\'     &  {Full Fine-tuning} & 41.60 & 27.47 & 42.33 & 42.75 & 56.25 & 78.86 & 48.21\\\\\\\\\\\\\\\\\\\\n\\', \\'     &  {\\\\\\\\ours} & \\\\\\\\textbf{47.60} & \\\\\\\\textbf{31.93} & \\\\\\\\textbf{52.77} & \\\\\\\\textbf{49.22} & \\\\\\\\textbf{62.00} & \\\\\\\\textbf{82.11} & \\\\\\\\textbf{54.27}\\\\\\\\\\\\\\\\\\\\n\\', \\'     &  {$\\\\\\\\Delta_{FF}$} & $\\\\\\\\uparrow 14.42\\\\\\\\%$ & $\\\\\\\\uparrow 16.24\\\\\\\\%$ & $\\\\\\\\uparrow 24.66\\\\\\\\%$ & $\\\\\\\\uparrow 15.13\\\\\\\\%$ & $\\\\\\\\uparrow 10.22\\\\\\\\%$ & $\\\\\\\\uparrow 4.12\\\\\\\\%$ & $\\\\\\\\uparrow 12.57\\\\\\\\%$\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'     \\\\n\\', \\'    \\\\\\\\multirow{5}{*}{\\\\\\\\begin{tabular}[c]{@{}c@{}}250M Student\\\\\\\\\\\\\\\\Size: 8.3\\\\\\\\%/3.6\\\\\\\\%\\\\\\\\end{tabular}} & Inference & 31.00 & 23.00 & 30.47 & 30.78 & 48.00 & 57.72 & 36.83 \\\\\\\\\\\\\\\\\\\\n\\', \\'    &  LoRA & 51.40 & 37.25 & 47.66 & 53.14 & 62.00 & 82.93 & 55.73\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {Full Fine-tuning} & 56.60 & 38.88 & 47.55 & 52.55 & 64.75 & 89.43 & 58.29\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {\\\\\\\\ours} & \\\\\\\\textbf{64.20} & \\\\\\\\textbf{47.98} & \\\\\\\\textbf{60.17} & \\\\\\\\textbf{60.78} & \\\\\\\\textbf{66.25} & \\\\\\\\textbf{90.24} & \\\\\\\\textbf{64.94}\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {$\\\\\\\\Delta_{FF}$} & $\\\\\\\\uparrow 13.43\\\\\\\\%$ & $\\\\\\\\uparrow 23.41\\\\\\\\%$ & $\\\\\\\\uparrow 26.54\\\\\\\\%$ & $\\\\\\\\uparrow 15.66\\\\\\\\%$ & $\\\\\\\\uparrow 2.32\\\\\\\\%$ & $\\\\\\\\uparrow 0.91\\\\\\\\%$ & $\\\\\\\\uparrow 11.40\\\\\\\\%$\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    \\\\n\\', \\'    \\\\\\\\multirow{5}{*}{\\\\\\\\begin{tabular}[c]{@{}c@{}}780M Student\\\\\\\\\\\\\\\\Size: 26.0\\\\\\\\%/11.1\\\\\\\\%\\\\\\\\end{tabular}} & Inference & 50.40 & 51.07  & 51.90 & 39.80 & 64.25 & 63.41 & 53.47\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  LoRA & 64.00 & 57.77 & 57.02 & 68.63 & 70.25 & 86.18 & 67.31\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {Full Fine-tuning} & 71.20 & 62.92 & 58.43 & 68.82 & 70.25 & 90.24 & 70.31\\\\\\\\\\\\\\\\\\\\n\\', \\'    & {\\\\\\\\ours} & \\\\\\\\textbf{74.40} & \\\\\\\\textbf{64.29} & \\\\\\\\textbf{67.90} & \\\\\\\\textbf{70.98} & \\\\\\\\textbf{73.00} & \\\\\\\\textbf{92.68} & \\\\\\\\textbf{73.88}\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {$\\\\\\\\Delta_{FF}$} & $\\\\\\\\uparrow 4.49\\\\\\\\%$ & $\\\\\\\\uparrow 2.18\\\\\\\\%$ & $\\\\\\\\uparrow 16.21\\\\\\\\%$ & $\\\\\\\\uparrow 3.14\\\\\\\\%$ & $\\\\\\\\uparrow 3.91\\\\\\\\%$ & $\\\\\\\\uparrow 2.70\\\\\\\\%$ & $\\\\\\\\uparrow 5.07\\\\\\\\%$\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\n\\', \\'    \\\\\\\\bottomrule\\\\n\\', \\'  \\\\\\\\end{tabular}\\\\n\\', \\'\\\\\\\\end{table*}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Learning a Small Student}\\\\n\\', \\'A straightforward strategy to incorporate rationales as supervision is to append each rationale \\\\\\\\(R_i^m\\\\\\\\) generated by the teacher models as supplementary input to the student model, along with the question \\\\\\\\(Q_i\\\\\\\\) and options \\\\\\\\(O_i\\\\\\\\). However, this method faces challenges due to limitations in computational resources at the inference stage, especially because rationales must be pre-generated for every data sample in both training and test sets~\\\\\\\\cite{wang2022pinto}. To overcome this issue, we employ rationales as a form of supervisory signal during the training process to develop a model that is adept at generating its own explanations. Subsequently, this trained model can be utilized on the test set, eliminating the need for pre-generated rationales to facilitate accurate reasoning. Specifically, \\\\\\\\ours integrates rationales from multiple teacher models into a unified multi-task instruction tuning framework. This necessitates the assignment of a unique prefix \\\\\\\\(p\\\\\\\\) for distinguishing between learning tasks from different teachers. The student model is trained not only to predict labels but also to generate rationales akin to those produced by the teachers. Accordingly, the overall loss function $\\\\\\\\mathcal{L}$ is as follows:\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'\\\\\\\\mathcal{L} = \\\\\\\\mathcal{L}_A + \\\\\\\\sum_{m=1}^{M}\\\\\\\\alpha^m\\\\\\\\mathcal{L}_{T^m}, \\\\n\\', \\'\\\\\\\\label{eq:total_loss}\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'where \\\\\\\\(\\\\\\\\mathcal{L}_A\\\\\\\\) denotes the objective of learning from ground truth answers, $\\\\\\\\mathcal{L}_{T^m}$ indicates the objective of learning from $m$-th teacher, $\\\\\\\\alpha^m$ is the importance weight for $T^m$, and $M$ is the number of teacher LLMs. Formally, $\\\\\\\\mathcal{L}_A$ and $\\\\\\\\mathcal{L}_{T^m}$ are defined as follows:\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'\\\\\\\\mathcal{L}_A = \\\\\\\\frac{1}{N} \\\\\\\\sum_{i=1}^{N} \\\\\\\\ell(S(Q_i, O_i, p_A; \\\\\\\\theta_S), A_i),\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'    \\\\\\\\mathcal{L}_{T^m} = \\\\\\\\frac{1}{N} \\\\\\\\sum_{i=1}^{N} \\\\\\\\ell(S(Q_i, O_i, p_m; \\\\\\\\theta_S), R_i^m),\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'where $N$ is the number of data samples, $\\\\\\\\ell$ indicates the cross-entropy loss between the predicted and target tokens. Here $\\\\\\\\mathcal{L}_A$ encourages the student $S$ to generate ground truth answer $A_i$ by minimizing the difference between it and the student output given the question $Q_i$, options $O_i$, and instruction prefix $p_A$ for generating answers. On the other hand, $\\\\\\\\mathcal{L}_T^m$ facilitates the student $S$ to mimic the reasoning capability of teacher $T^m$ by learning from its rationale $R_i^m$, with the guidance of instruction prefix $p_m$ for $T^m$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\section{Experiments}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Experimental Setup}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\textbf{Datasets.} \\\\n\\', \\'We follow the setup in GNP \\\\\\\\cite{gnp} for the usage of commonsense reasoning and biomedical reasoning datasets, including OpenBookQA (OBQA)~\\\\\\\\cite{OpenBookQA2018}, The AI2 Reasoning Challenge (ARC)~\\\\\\\\cite{Clark2018ThinkYH}, Physical Interaction Question Answering (PIQA)~\\\\\\\\cite{Bisk2020PIQA}, RiddleSense (Riddle)~\\\\\\\\cite{lin-etal-2021-riddlesense}, PubMedQA (PQA)~\\\\\\\\cite{jin2019pubmedqa}, and BioASQ~\\\\\\\\cite{Tsatsaronis2015BIOASQ}.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Baselines.} \\\\n\\', \"We benchmark \\\\\\\\ours against the teacher\\'s performance and various training approaches, including an Inference configuration without training, an efficient training method LoRA \\\\\\\\cite{hu2022lora} that updates a subset of parameters, and the full fine-tuning that updates all the parameters in the student.\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Implementation Details.} For \\\\\\\\ours, we set the learning rate to \\\\\\\\(5 \\\\\\\\times 10^{-5}\\\\\\\\), batch size to 16, maximum input length to 1024, and epoch to 1. Trade-off weights \\\\\\\\(\\\\\\\\alpha_{T_n}\\\\\\\\) are explored within \\\\\\\\{0.01, 0.1, 0.5, 1, 2, 3\\\\\\\\}. For the choice of LLMs, we use FLAN-T5 ~\\\\\\\\cite{chung2022scaling} small (80M), base (250M), and large (780M) as the student, and FLAN-T5 xlarge (3B) and LLaMA 2-chat ~\\\\\\\\cite{touvron2023llama2} (7B) as teachers. Experiments are conducted on four NVIDIA Tesla H100 GPUs.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Performance Comparison}\\\\n\\', \\'\\\\\\\\textbf{Comparison to Students Learning Methods.}\\\\n\\', \\'The results of six datasets and two reasoning tasks are shown in Table ~\\\\\\\\ref{Overall}. From the table, we observe that the employment of a full-finetuning method, despite its theoretically enhanced capacity for parameter adjustment, does not consistently yield superior results to LoRA. Conversely, \\\\\\\\ours demonstrates substantial performance enhancements across all datasets and LLM sizes. In total, \\\\\\\\ours achieves an average enhancement of \\\\\\\\textbf{+12.57\\\\\\\\%}, \\\\\\\\textbf{+11.40\\\\\\\\%} and \\\\\\\\textbf{+5.07\\\\\\\\%} for students with 80M, 250M, and 780M parameters, respectively. This validates the effectiveness of \\\\\\\\ours and underscores the importance and benefits of learning from teachers.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Comparison to Teachers.} \\\\n\\', \\'\\\\\\\\ours also shows superior performance compared to the teacher models. For example, a 780M student can achieve an average performance of 73.88 across different datasets, which is \\\\\\\\textbf{+14.56\\\\\\\\%} better than the 3B teacher and \\\\\\\\textbf{+23.40\\\\\\\\%} better than the 7B teacher. Moreover, an even smaller student model with 250M parameters can outperform the teachers (\\\\\\\\textbf{+0.70\\\\\\\\%} to 3B, \\\\\\\\textbf{+16.82\\\\\\\\%} to 7B) while using only \\\\\\\\textbf{8.3\\\\\\\\%} and \\\\\\\\textbf{3.6\\\\\\\\%} of the teacher parameters.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\subsection{Ablation Study}\\\\n\\', \\'For a comprehensive evaluation, we conduct ablation studies to validate the contribution of the in-context example generator and rationales from multiple teachers. To facilitate this, we create three ablation variants of \\\\\\\\ours. \\\\n\\', \\'\\\\\\\\begin{itemize}[leftmargin=*]\\\\n\\', \\'    \\\\\\\\item \\\\\\\\textbf{w/o in-context} rules out the in-context examples during rationale generation.\\\\n\\', \\'    \\\\\\\\item \\\\\\\\textbf{w/o LLaMa} and \\\\\\\\textbf{w/o T5} exclude the rationale supervision from the corresponding teacher model during distillation.\\\\n\\', \\'\\\\\\\\end{itemize}\\\\n\\', \\'Table ~\\\\\\\\ref{tab:results} depicts the comparison between the full \\\\\\\\ours model and its ablations on the six datasets. According to the table, the decreasing performances of removing the in-context examples and teacher rationales demonstrate their effectiveness in enhancing the model. By combining all the designs, the proposed \\\\\\\\ours achieves the best performance. These findings indicate that learning high-quality rationale signals from multiple teachers is beneficial in distilling a small LLM with better reasoning ability.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{table}[t]\\\\n\\', \\'  \\\\\\\\caption{Impact of in-context examples and contributions of various teachers.}\\\\n\\', \\'  \\\\\\\\vspace{-0.1in}\\\\n\\', \\'  \\\\\\\\label{tab:results}\\\\n\\', \\'  \\\\\\\\resizebox{\\\\\\\\linewidth}{!}{\\\\n\\', \\'  \\\\\\\\begin{tabular}{ccccccc}\\\\n\\', \\'    \\\\\\\\toprule\\\\n\\', \\'    &  \\\\\\\\multicolumn{4}{c}{Commonsense} & \\\\\\\\multicolumn{2}{c}{Biomedical} \\\\\\\\\\\\\\\\ \\\\n\\', \\'    \\\\\\\\cmidrule{2-7}\\\\n\\', \\'    \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{Variant}} & {\\\\\\\\textbf{OBQA}} & {\\\\\\\\textbf{ARC}} & {\\\\\\\\textbf{PIQA}} & {\\\\\\\\textbf{Riddle}} & {\\\\\\\\textbf{PQA}} & {\\\\\\\\textbf{BioASQ}} \\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    {w/o in-context}  & 73.20 & 63.09 & 66.27 & 69.22 & 70.75 & 86.99 \\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\n\\', \\'    {w/o LLaMA}  & 73.00 & 62.32 & 66.70 & 68.82 & 69.25 & 87.81 \\\\\\\\\\\\\\\\\\\\n\\', \\'\\\\n\\', \\'    {w/o T5}  & 73.80 & 61.80 & 66.49 & 68.63 & 69.50 & 88.62 \\\\\\\\\\\\\\\\\\\\n\\', \\'\\\\n\\', \\'    {\\\\\\\\ours} & \\\\\\\\textbf{74.40} & \\\\\\\\textbf{64.29} & \\\\\\\\textbf{67.90} & \\\\\\\\textbf{70.98} & \\\\\\\\textbf{73.00} & \\\\\\\\textbf{92.68} \\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\bottomrule\\\\n\\', \\'  \\\\\\\\end{tabular}\\\\n\\', \\'  }\\\\n\\', \\'  \\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\end{table}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\subsection{Parameter Sensitivity}\\\\n\\', \\'\\\\n\\', \"The exploration of trade-off weights reveals the model\\'s adaptability across different choices of parameter values. We conduct sensitivity experiments on datasets ARC for commonsense reasoning and PQA for biomedical reasoning in ~\\\\\\\\ref{fig:sensitivity}. According to the figure, we observe that the optimal parameters for various datasets and tasks differ. The reason for this phenomenon is that biomedical reasoning questions are often lengthy and complex, weakening the impact of rationales from teachers and making a small value of $\\\\\\\\alpha$ sufficient. In contrast, commonsense reasoning questions are typically concise and straightforward, making the rationales from teacher models valuable and leading to a large value of $\\\\\\\\alpha$.\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{figure}[t]\\\\n\\', \\'  \\\\\\\\centering\\\\n\\', \\'  \\\\\\\\includegraphics[width=\\\\\\\\columnwidth]{Figures/param_sen.png}\\\\n\\', \\'  \\\\\\\\vspace{-0.25in}\\\\n\\', \\'  \\\\\\\\caption{Performance w.r.t. different values of weight $\\\\\\\\alpha$.}\\\\n\\', \\'  \\\\\\\\vspace{-0.15in}\\\\n\\', \\'  \\\\\\\\label{fig:sensitivity}\\\\n\\', \\'\\\\\\\\end{figure}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\section{Conclusion}\\\\n\\', \\'In this paper, we propose \\\\\\\\ours, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. \\\\\\\\ours involves several innovative designs, such as inheriting a broader range of knowledge across different teachers and learning contextually appropriate and accurate rationales using an in-context example generator along with a teacher-forcing Chain-of-Thought strategy. Extensive experiments across six datasets and two reasoning tasks validate the superiority of \\\\\\\\ours.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\bibliographystyle{acm}\\\\n\\', \\'\\\\\\\\bibliography{reference}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\end{document}\\\\n\\']\\n      Analyse paper by section and strictly follow this template but json format:\\n      \\n    {Section Name:  Introduction,\\n    Slide Information: {\\n      Number of Slides: 3,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Introduction,\\n      Image: Figure path if it is present in Introduction section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n    {Section Name:  Method,\\n    Slide Information: {\\n      Number of Slides: 2,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Method,\\n      Image: Figure path if it is present in Method section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n    {Section Name:  Experiments,\\n    Slide Information: {\\n      Number of Slides: 1,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Experiments,\\n      Image: Figure path if it is present in Experiments section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n    {Section Name:  Conclusion,\\n    Slide Information: {\\n      Number of Slides: 1,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Conclusion,\\n      Image: Figure path if it is present in Conclusion section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n      '}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "x3OkMeJ1jwOZ"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key  = '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'\n",
        "\n",
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-4-turbo-preview\",\n",
        "                                 response_format={ \"type\": \"json_object\" },\n",
        "                                 temperature=0, max_tokens=1200):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=get_completion_from_messages(messages=messages)"
      ],
      "metadata": {
        "id": "VnKJ8ONlkB8B"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def LLM_Output_to_dict(LLM_Output,Content_Num):\n",
        "    Section_Json = []\n",
        "    for x in LLM_Output.split('\\n\\n'):\n",
        "        Section_Json.append(x.replace('```','').lstrip().rstrip().lstrip('json'))\n",
        "    Section_Json = [x for x in Section_Json if not x == '']\n",
        "    result = json.loads(f'{Section_Json[Content_Num]}')\n",
        "    Section_Name = result['Section Name']\n",
        "    Number_of_Slides = result['Slide Information']['Number of Slides']\n",
        "    Slide_Info = [x for x in result['Slide Information']['Speaker Notes'].values()]\n",
        "    Table = result['Slide Information']['Table']\n",
        "    Image = result['Slide Information']['Image']\n",
        "    Generative_Prompt = result['Slide Information']['Generative Prompt']\n",
        "    return Section_Name,Number_of_Slides,Slide_Info,Table,Image,Generative_Prompt\n",
        "\n",
        "\n",
        "\n",
        "Section_Name,Number_of_Slides,Slide_Info,Table,Image,Generative_Prompt = LLM_Output_to_dict(output,0)\n",
        "print(Section_Name)\n",
        "print(Number_of_Slides)\n",
        "print(Slide_Info)\n",
        "print(Table)\n",
        "print(Image)\n",
        "print(Generative_Prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq55JgJHhRDP",
        "outputId": "ba099b95-d560-4cd9-a0dc-a16505d40075"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduction\n",
            "3\n",
            "['Welcome to our presentation on TinyLLM, a novel approach to learning small student language models from multiple large teacher language models. The motivation behind our work is the increasing need for deploying flexible and less expensive language models without compromising their reasoning capabilities. Large language models have shown remarkable success across various domains, but their deployment is often hindered by their size and the computational resources they require.', 'Our research identifies two main problems with existing methods of distilling knowledge into smaller models: limited knowledge diversity and the lack of rich contextual information. By relying on a single teacher model, previous approaches have confined the learning scope of the student model, limiting its potential. Furthermore, the absence of rationale behind the correct answers has been a significant gap in effective reasoning and understanding.', 'To address these challenges, we introduce TinyLLM, a paradigm that leverages knowledge from multiple large teacher models to enhance the reasoning ability of a small student model. Our method not only focuses on generating the correct answers but also on understanding the rationales behind these answers. This approach allows the student model to inherit a broader range of skills and knowledge, leading to better generalization capabilities.']\n",
            "None\n",
            "None\n",
            "Abstract concept art of knowledge transfer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFZ9Eo-Vm2QX",
        "outputId": "abd16208-14c0-4c95-dd0c-7bf9c4292c23"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"Section Name\": \"Introduction\",\n",
            "  \"Slide Information\": {\n",
            "    \"Number of Slides\": 3,\n",
            "    \"Speaker Notes\": {\n",
            "      \"Slide 1\": \"In recent years, the development of Large Language Models (LLMs) has significantly advanced, impacting various domains such as society, education, and scientific understanding. However, despite the promising capabilities of larger models like GPT-4 and Claude-2, their smaller counterparts struggle to perform complex reasoning tasks. This challenge is attributed to the scaling law of LLMs, which suggests a performance gap between larger and smaller models.\",\n",
            "      \"Slide 2\": \"To bridge this gap, knowledge distillation has emerged as a powerful tool, allowing the transfer of capabilities from larger to smaller models. DistilBERT, Alpaca, and Vicuna are notable examples of effective distillation methods. Yet, current approaches face limitations, including restricted knowledge diversity and a lack of rich contextual information, which hinder the learning process of smaller models.\",\n",
            "      \"Slide 3\": \"Addressing these challenges, we introduce TinyLLM, a novel knowledge distillation paradigm that leverages multiple large teacher LLMs to enhance the reasoning ability of a small student LLM. By incorporating diverse knowledge sources and focusing on understanding the rationales behind answers, TinyLLM aims to overcome the existing limitations and significantly improve the performance of smaller LLMs.\"\n",
            "    },\n",
            "    \"Table\": null,\n",
            "    \"Image\": \"Figures/pipeline.png\",\n",
            "    \"Generative Prompt\": \"Design a slide background that visually represents the concept of knowledge transfer from multiple large language models to a smaller model, emphasizing diversity and the importance of understanding rationales.\"\n",
            "  }\n",
            "}\n",
            "\n",
            "{\n",
            "  \"Section Name\": \"Method\",\n",
            "  \"Slide Information\": {\n",
            "    \"Number of Slides\": 2,\n",
            "    \"Speaker Notes\": {\n",
            "      \"Slide 1\": \"The TinyLLM method involves a novel approach to knowledge distillation, focusing on learning from multiple teacher LLMs. We introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy. These components ensure that the student model not only generates correct answers but also understands the reasoning behind them. By leveraging diverse reasoning skills from various teacher LLMs, TinyLLM aims to provide a more comprehensive learning experience.\",\n",
            "      \"Slide 2\": \"To effectively distill knowledge from multiple teachers, TinyLLM employs a multi-teacher architecture. Each teacher model contributes unique rationales for given questions, which are then integrated into the student model's learning process. This approach allows the student model to inherit a broader range of skills and knowledge, leading to better generalization capabilities and improved performance across different reasoning tasks.\"\n",
            "    },\n",
            "    \"Table\": null,\n",
            "    \"Image\": \"Figures/pipeline.png\",\n",
            "    \"Generative Prompt\": \"Create a slide background that illustrates the process of integrating knowledge from multiple teachers into a single student model, highlighting the in-context example generator and the teacher-forcing Chain-of-Thought strategy.\"\n",
            "  }\n",
            "}\n",
            "\n",
            "{\n",
            "  \"Section Name\": \"Experiments\",\n",
            "  \"Slide Information\": {\n",
            "    \"Number of Slides\": 1,\n",
            "    \"Speaker Notes\": {\n",
            "      \"Slide 1\": \"Our experimental evaluation of TinyLLM involved six datasets across commonsense and biomedical reasoning tasks. The results demonstrate TinyLLM's superior performance, with significant improvements over full fine-tuning methods and even outperforming the large teacher LLMs in some cases. These findings validate the effectiveness of our approach, showcasing the benefits of learning from multiple teachers and the importance of understanding rationales in knowledge distillation.\"\n",
            "    },\n",
            "    \"Table\": {\n",
            "      \"header\": [\"LLM\", \"Method\", \"OBQA\", \"ARC\", \"PIQA\", \"Riddle\", \"PQA\", \"BioASQ\", \"Total\"],\n",
            "      \"rows\": [\n",
            "        [\"3B/7B Teacher\", \"FLAN-T5 xlarge\", \"69.20\", \"68.24\", \"58.43\", \"53.73\", \"71.50\", \"65.85\", \"64.49\"],\n",
            "        [\"80M Student\", \"TinyLLM\", \"47.60\", \"31.93\", \"52.77\", \"49.22\", \"62.00\", \"82.11\", \"54.27\"],\n",
            "        [\"250M Student\", \"TinyLLM\", \"64.20\", \"47.98\", \"60.17\", \"60.78\", \"66.25\", \"90.24\", \"64.94\"],\n",
            "        [\"780M Student\", \"TinyLLM\", \"74.40\", \"64.29\", \"67.90\", \"70.98\", \"73.00\", \"92.68\", \"73.88\"]\n",
            "      ]\n",
            "    },\n",
            "    \"Image\": null,\n",
            "    \"Generative Prompt\": \"Design a slide background that visually represents the experimental evaluation process, highlighting the comparison between TinyLLM and traditional fine-tuning methods across various datasets.\"\n",
            "  }\n",
            "}\n",
            "\n",
            "{\n",
            "  \"Section Name\": \"Conclusion\",\n",
            "  \"Slide Information\": {\n",
            "    \"Number of Slides\": 1,\n",
            "    \"Speaker Notes\": {\n",
            "      \"Slide 1\": \"In conclusion, TinyLLM represents a significant advancement in the field of knowledge distillation for language models. By learning from multiple large teacher LLMs and focusing on the understanding of rationales, TinyLLM overcomes the limitations of existing methods, offering improved flexibility, efficiency, and performance. Our experiments across diverse reasoning tasks confirm the effectiveness of this approach, paving the way for future research in compact language model training.\"\n",
            "    },\n",
            "    \"Table\": null,\n",
            "    \"Image\": null,\n",
            "    \"Generative Prompt\": \"Create a slide background that encapsulates the essence of TinyLLM's contribution to knowledge distillation, emphasizing the breakthrough in learning from multiple teachers and the focus on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stable Diffusion"
      ],
      "metadata": {
        "id": "24Snr_5Q6HGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stability-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AfdGXRA71nc",
        "outputId": "6e28d266-8f3d-4c57-fa87-0ae0ed58f147"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stability-sdk\n",
            "  Downloading stability_sdk-0.8.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from stability-sdk) (9.4.0)\n",
            "Collecting grpcio==1.53.0 (from stability-sdk)\n",
            "  Downloading grpcio-1.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-tools==1.53.0 (from stability-sdk)\n",
            "  Downloading grpcio_tools-1.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from stability-sdk)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from stability-sdk) (2.0.2)\n",
            "Collecting protobuf==4.21.12 (from stability-sdk)\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m409.8/409.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools==1.53.0->stability-sdk) (67.7.2)\n",
            "Installing collected packages: python-dotenv, protobuf, grpcio, grpcio-tools, stability-sdk\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.62.0\n",
            "    Uninstalling grpcio-1.62.0:\n",
            "      Successfully uninstalled grpcio-1.62.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.53.0 grpcio-tools-1.53.0 protobuf-4.21.12 python-dotenv-1.0.1 stability-sdk-0.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import warnings\n",
        "from PIL import Image\n",
        "from stability_sdk import client\n",
        "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation"
      ],
      "metadata": {
        "id": "hlJ4pz7i8RDR"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stability_api = client.StabilityInference(\n",
        "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
        "    verbose=True, # Print debug messages.\n",
        "    engine=\"stable-diffusion-xl-1024-v1-0\", # Set the engine to use for generation.\n",
        "    # Check out the following link for a list of available engines: https://platform.stability.ai/docs/features/api-parameters#engine\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhTV_sM28E1h",
        "outputId": "970cbbda-8b28-456f-e198-3c75af34811a"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stability_sdk.client:Opening channel to grpc.stability.ai:443\n",
            "INFO:stability_sdk.client:Channel opened to grpc.stability.ai:443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers = stability_api.generate(\n",
        "    prompt= Generative_Prompt,\n",
        "    seed=4253978046, # If a seed is provided, the resulting generated image will be deterministic.\n",
        "                     # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
        "                     # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
        "    steps=50, # Amount of inference steps performed on image generation. Defaults to 30.\n",
        "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
        "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
        "                   # Defaults to 7.0 if not specified.\n",
        "    width=1024, # Generation width, defaults to 512 if not included.\n",
        "    height=1024, # Generation height, defaults to 512 if not included.\n",
        "    samples=1, # Number of images to generate, defaults to 1 if not included.\n",
        "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
        "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
        "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m, k_dpmpp_sde)\n",
        ")"
      ],
      "metadata": {
        "id": "gEmXTl9d8Zv5"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for resp in answers:\n",
        "    for artifact in resp.artifacts:\n",
        "        if artifact.finish_reason == generation.FILTER:\n",
        "            warnings.warn(\n",
        "                \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                \"Please modify the prompt and try again.\")\n",
        "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "            img = Image.open(io.BytesIO(artifact.binary))\n",
        "            img.save(str(artifact.seed)+ \".png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX1T_M7H8kDy",
        "outputId": "bcd1796d-f3ce-4d30-c24a-bca940748edc"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stability_sdk.client:Sending request.\n",
            "INFO:stability_sdk.client:Got answer  with artifact types ['ARTIFACT_IMAGE'] in 8.80s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "No53PB4l8YUX"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdlSIDmJ6K8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}