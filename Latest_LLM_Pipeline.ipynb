{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LExaPuw6oGeZ",
        "outputId": "1a74d6eb-f6fe-426c-8a70-3a5ab8bd2e16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "with open('complete_tex.tex','r') as f:\n",
        "  latex_string = f.readlines()\n",
        "\n",
        "figure_list = []\n",
        "Section_headings = {}\n",
        "for count,lines in enumerate(latex_string):\n",
        "  # print(lines)\n",
        "  if re.match(r'\\\\section', lines):\n",
        "    head = lines.lstrip(r'\\\\section{').rstrip().rstrip('}')\n",
        "    Section_headings[head] = 0\n",
        "  if re.search(r'\\\\includegraphics', lines):\n",
        "    try:\n",
        "      figure_list.append(lines.split(']{')[1].rstrip().rstrip('}'))\n",
        "    except:\n",
        "      pass\n",
        "print(figure_list)\n",
        "print(Section_headings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_yheF7P4CDQ",
        "outputId": "14dea654-26fb-43e5-c9be-9a1fdde30d59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Figures/pipeline.png', 'Figures/param_sen.png']\n",
            "{'Introduction': 0, 'Method': 0, 'Experiments': 0, 'Conclusion': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prompt_builder(title,page_length,count):\n",
        "    return f'''\n",
        "    {{Section Name:  {title},\n",
        "    Slide Information: {{\n",
        "      Number of Slides: {page_length},\n",
        "      Speaker Notes: {{\"Slide {{Number}}\": 150 words speech for slide}},\n",
        "      Table: Latex Table to header and row dictionary if present in {title},\n",
        "      Image: Figure path if it is present in {title} section (possible selections {figure_list}),\n",
        "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
        "    }}\n",
        "    }}\n",
        "    '''\n",
        "\n",
        "user_input = input('Enter the page length for each section: ')\n",
        "user_input = user_input.split(',')\n",
        "print(user_input)\n",
        "\n",
        "for count,key in enumerate(Section_headings):\n",
        "  Section_headings[key] = user_input[count]\n",
        "\n",
        "prompt_base = ''\n",
        "for count,(key,val) in enumerate(Section_headings.items()):\n",
        "  prompt_base+=prompt_builder(key,val,count+1)\n",
        "\n",
        "print(prompt_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKfBJ1woZ5e1",
        "outputId": "23012690-9bd9-4de9-d554-5cc1682e40c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the page length for each section: 3,2,1,1\n",
            "['3', '2', '1', '1']\n",
            "\n",
            "    {Section Name:  Introduction,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 3,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Introduction,\n",
            "      Image: Figure path if it is present in Introduction section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n",
            "    {Section Name:  Method,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 2,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Method,\n",
            "      Image: Figure path if it is present in Method section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n",
            "    {Section Name:  Experiments,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 1,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Experiments,\n",
            "      Image: Figure path if it is present in Experiments section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n",
            "    {Section Name:  Conclusion,\n",
            "    Slide Information: {\n",
            "      Number of Slides: 1,\n",
            "      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\n",
            "      Table: Latex Table to header and row dictionary if present in Conclusion,\n",
            "      Image: Figure path if it is present in Conclusion section (possible selections ['Figures/pipeline.png', 'Figures/param_sen.png']),\n",
            "      Generative Prompt: Propose a 10 words prompt for simple slide background\n",
            "    }\n",
            "    }\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are an expert slide expert that extracts latex papers and create presentation slides.\",\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": f'''\n",
        "      Given the document: {latex_string}\n",
        "      Analyse paper by section and strictly follow this template but json format:\n",
        "      {prompt_base}\n",
        "      '''\n",
        "    }\n",
        "  ]\n",
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvYXc0graMUk",
        "outputId": "38c9b420-5cbe-4fa7-d9a4-6847f441170b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are an expert slide expert that extracts latex papers and create presentation slides.'}, {'role': 'user', 'content': '\\n      Given the document: [\\'\\\\\\\\documentclass[sigconf,nonacm]{acmart}\\\\n\\', \\'\\\\\\\\makeatletter\\\\n\\', \\'\\\\\\\\renewcommand\\\\\\\\@formatdoi[1]{\\\\\\\\ignorespaces}\\\\n\\', \\'\\\\\\\\makeatother\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\AtBeginDocument{\\\\\\\\providecommand\\\\\\\\BibTeX{{\\\\\\\\normalfont B\\\\\\\\kern-0.5em{\\\\\\\\scshape i\\\\\\\\kern-0.25em b}\\\\\\\\kern-0.8em\\\\\\\\TeX}}}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\acmDOI{}\\\\n\\', \\'\\\\\\\\acmISBN{}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\usepackage{hyperref}\\\\n\\', \\'\\\\\\\\usepackage{siunitx}\\\\n\\', \\'\\\\\\\\usepackage{xspace}\\\\n\\', \\'\\\\\\\\usepackage{multirow}\\\\n\\', \\'\\\\\\\\usepackage{enumitem}\\\\n\\', \\'\\\\\\\\usepackage{placeins}\\\\n\\', \\'\\\\\\\\usepackage{natbib}\\\\n\\', \\'\\\\\\\\newcommand{\\\\\\\\ours}{\\\\\\\\textsc{TinyLLM}\\\\\\\\xspace}\\\\n\\', \\'\\\\\\\\settopmatter{printacmref=false}\\\\n\\', \\'\\\\\\\\setcopyright{none}\\\\n\\', \\'\\\\\\\\pagestyle{plain}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{document}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\title{\\\\\\\\ours: Learning a Small Student from Multiple Large Language Models}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Yijun Tian}\\\\n\\', \\'\\\\\\\\authornote{Equally contributed.}\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of Notre Dame}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{yijun.tian@nd.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Yikun Han}\\\\n\\', \\'\\\\\\\\authornotemark[1]\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of Michigan}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{yikunhan@umich.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Xiusi Chen}\\\\n\\', \\'\\\\\\\\authornotemark[1]\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of California, Los Angeles}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{xchen@cs.ucla.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Wei Wang}\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of California, Los Angeles}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{weiwang@cs.ucla.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\author{Nitesh V. Chawla}\\\\n\\', \\'\\\\\\\\affiliation{\\\\\\\\institution{University of Notre Dame}\\\\n\\', \\'  \\\\\\\\country{}\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\email{nchawla@nd.edu}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{abstract}\\\\n\\', \\'  Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose \\\\\\\\ours, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that \\\\\\\\ours can outperform large teacher LLMs significantly, despite having a considerably smaller model size.\\\\n\\', \\'\\\\\\\\end{abstract}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\maketitle\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\section{Introduction}\\\\n\\', \\'Large language models (LLMs) have recently taken over various domains and applications, including society ~\\\\\\\\cite{rao2023makes}, education~\\\\\\\\cite{zelikman2023generating}, and scientific understanding~\\\\\\\\cite{beltagy2019scibert}. Despite the success of larger emerging models (GPT-4, Claude-2), their smaller counterparts hardly demonstrate such promising capabilities for performing complex reasoning~\\\\\\\\cite{wei2022emergentabilities,chen2023gotta}. This has been unveiled as the well-known scaling law of LLMs~\\\\\\\\cite{kaplan2020scalinglaws}. As such, it has been desirable to transfer the capabilities of the larger models to the smaller ones so that the smaller ones could be easily deployed while still enjoying the strong capabilities. Previous studies have shown that knowledge distillation is an instrumental tool in mitigating the performance gap between larger models such as LLMs and smaller ones~\\\\\\\\cite{wan2023efficient, hsieh-etal-2023-distilling, kd_survey}. Examples of effective distillation methods include DistilBERT~\\\\\\\\cite{sanh2019distilbert}, Alpaca~\\\\\\\\cite{alpaca} and Vicuna~\\\\\\\\cite{zheng2023judging}.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'However, existing methods suffer from two major drawbacks: (1) \\\\\\\\textbf{Limited Knowledge Diversity}: Current research predominantly employs a single-teacher approach, which confines the learning scope of the student model to the knowledge derived from its own training and architecture designs \\\\\\\\cite{ho2022largelanguagemodels,magister2022teachingsmall,li2023symbolicchain,wang2022pinto}. This means that the student model is limited to the perspectives, biases, and potential weaknesses of the teacher. (2) \\\\\\\\textbf{Lack of Rich Contextual Information}: While rationales play a vital role in effective reasoning \\\\\\\\cite{wei2022chainofthought,kojima2022zero}, current research primarily focuses on leveraging ground truth labels, which indicate the correct answer but do not provide insights into the reasoning and thought process behind that answer.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'In response to the above issues, we propose \\\\\\\\ours, a paradigm that improves the reasoning ability of small student LLMs by distilling knowledge from multiple large teacher LLMs with rationale guidance. Specifically, \\\\\\\\ours mitigates the limited knowledge diversity issue by involving multiple teacher models as \\\\\\\\textit{co-advisors}. To fully exploit each teacher model and mitigate the lack of rich contextual information problem, \\\\\\\\ours asks the teacher for the rationales to support the answers. By learning from multiple teachers, the student model can inherit a broader range of skills and knowledge, leading to better generalization capabilities. In addition, to ensure the rationales are grounded in contextually appropriate scenarios and reflect the true underlying reasoning procedure, \\\\\\\\ours features an in-context example generator and a teacher-forcing Chain-of-Thought strategy, making the teachers understand the task through demonstrations and generate the accurate rationales.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'To thoroughly assess our approach, we conduct experiments on six datasets in commonsense and biomedical reasoning tasks. The results show that the usage of our paradigm enhances performance by \\\\\\\\textbf{+5.07\\\\\\\\%} to \\\\\\\\textbf{+12.57\\\\\\\\%} compared to full fine-tuning with significantly smaller model size, i.e., \\\\\\\\textbf{1.1\\\\\\\\%} to \\\\\\\\textbf{26.0\\\\\\\\%} of teacher sizes. We also perform ablation studies to demonstrate the validity of teacher rationales and undertake hyperparameter analyses for a comprehensive evaluation. To summarize, our main contributions are as follows:\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{figure*}[ht]\\\\n\\', \\'\\\\\\\\begin{center}\\\\n\\', \\'\\\\\\\\includegraphics[width=\\\\\\\\textwidth]{Figures/pipeline.png}\\\\n\\', \\'\\\\\\\\end{center}\\\\n\\', \\'\\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\caption{\\\\n\\', \\'Pipeline of \\\\\\\\ours: Given an input question, we first generate in-context examples and obtain rationales from multiple large LLMs via a teacher-forcing Chain-of-Thought Strategy. Later, a small student LLM is trained to integrate rationales from different teachers via multi-task instruction tuning, along with the ground truth label.\\\\n\\', \\'}\\\\n\\', \\'\\\\\\\\label{fig:pipeline}\\\\n\\', \\'\\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\end{figure*}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{itemize}[nosep,leftmargin=*]\\\\n\\', \\'\\\\\\\\item We identify two problems with existing work on learning smaller language models: 1) limited knowledge diversity and 2) lack of rich contextual information.\\\\n\\', \\'\\\\\\\\item To address the problems, we propose \\\\\\\\ours, a novel knowledge distillation paradigm to learn a small student LLM by transferring reasoning capabilities from multiple large teacher LLMs. We encourage the student LLM to understand the rationale behind the generated answer.\\\\n\\', \\'\\\\\\\\item Extensive experiments validate the superiority of \\\\\\\\ours across six datasets and two reasoning tasks. \\\\n\\', \\'\\\\\\\\ours can achieve up to \\\\\\\\textbf{+12.57\\\\\\\\%} of performance improvement with 1.1\\\\\\\\% of model size.\\\\n\\', \\'\\\\\\\\end{itemize}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\section{Method}\\\\n\\', \\'The pipeline of \\\\\\\\ours is shown in Figure \\\\\\\\ref{fig:pipeline}.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\subsection{Preliminary}\\\\n\\', \\'\\\\\\\\textbf{Multiple Choice Question Answering.} A $k$-way multiple choice question answering (MCQA) is defined as follows: Given a question $Q_i$, a set of candidate answer options $O_i=\\\\\\\\{O_{i1},O_{i2},...,O_{ik}\\\\\\\\}$, the model is tasked with selecting the correct answer from the set $O_i$, such that the selected answer aligns the ground truth label $A_i$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Knowledge Distillation.} \\\\n\\', \\'The knowledge distillation process begins with the teacher model, denoted as $T$ parameterized by $\\\\\\\\theta_T$, which has been pre-trained on a large corpus. Later, the student model, $S$, with parameter $\\\\\\\\theta_S$, is tasked with distilling knowledge directly from $T$, leveraging the strong capabilities of $T$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Obtaining Rationales from Teachers}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent\\\\n\\', \\'\\\\\\\\textbf{In-context Example Generator.}\\\\n\\', \\'To enable the rationales that are generated by teachers to be grounded in contextually appropriate scenarios, we introduce an optional in-context example generator. This tool is designed to produce in-context examples for any given input, providing more detailed information about the input data and task. For simplicity, we select the examples randomly within the same dataset. This aids the teacher LLMs in comprehending the nature and specifics of the task more deeply. By integrating this generator, we facilitate a more informed and nuanced generation of rationales by the teacher models, enhancing the learning experience for the student model. \\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent\\\\n\\', \\'\\\\\\\\textbf{Teacher-forcing Chain-of-Thought.}\\\\n\\', \\'In addition, we design a teacher-forcing strategy to ensure the validity of the rationales. Compared to existing methods that simply employ regular chain-of-thought (CoT) mechanisms \\\\\\\\cite{wei2022chainofthought,kojima2022zero}, wherein an LLM is prompted with sets of questions and options \\\\\\\\(\\\\\\\\{Q_i, O_i\\\\\\\\}\\\\\\\\) to elicit rationales \\\\\\\\(R_i\\\\\\\\) directly, \\\\\\\\ours posits a distinct advantage in integrating the correct answer \\\\\\\\(A_i\\\\\\\\) into the input. We hypothesize that the inclusion of \\\\\\\\(A_i\\\\\\\\) alongside \\\\\\\\(Q_i\\\\\\\\) and \\\\\\\\(O_i\\\\\\\\) facilitates a more nuanced understanding of the input context and the correct logical rationales leading to the answer, thereby facilitating a more informed and accurate generation process. Specifically, we consider the concatenation of questions, options, and answers \\\\\\\\(\\\\\\\\{Q_i, O_i, A_i\\\\\\\\}\\\\\\\\) as the input to LLMs.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent\\\\n\\', \\'\\\\\\\\textbf{Rationales from Multiple Teachers.}\\\\n\\', \\'Given $M$ teachers, \\\\\\\\ours pioneers the usage of a multi-teacher architecture in which each teacher \\\\\\\\(T^m\\\\\\\\) is an LLM. In particular, the rationale \\\\\\\\(R_i^m\\\\\\\\) produced by a specific teacher model $\\\\\\\\theta_{T^m}$ for the \\\\\\\\(i\\\\\\\\)th question is derived using the question $Q_i$, options $O_i$, correct answer $A_i$, and in-context examples $P_i$. The process is formalized as follows:\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'R_i^m = T^m(Q_i, O_i, A_i, P_i; \\\\\\\\theta_{T^m}).\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{table*}[ht]\\\\n\\', \\'\\\\\\\\caption{Overall experimental results. The best results across different LLM sizes are highlighted in bold. $\\\\\\\\Delta_{FF}$ represents the relative performance improvement of \\\\\\\\ours to Full Fine-Tuning. Accuracy is used as the evaluation metric.}\\\\n\\', \\'\\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\label{Overall}\\\\n\\', \\'  \\\\\\\\begin{tabular}{ccccccccc}\\\\n\\', \\'    \\\\\\\\toprule\\\\n\\', \\'     &  & \\\\\\\\multicolumn{4}{c}{Commonsense Reasoning} & \\\\\\\\multicolumn{2}{c}{Biomedical Reasoning} & \\\\\\\\\\\\\\\\ \\\\n\\', \\'    \\\\\\\\cmidrule{3-8}\\\\n\\', \\'    \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{LLM}}  & \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{Method}} & {\\\\\\\\textbf{OBQA}} & {\\\\\\\\textbf{ARC}} & {\\\\\\\\textbf{PIQA}} & {\\\\\\\\textbf{Riddle}} & {\\\\\\\\textbf{PQA}} & {\\\\\\\\textbf{BioASQ}} & \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{Total}}\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    \\\\\\\\multirow{2}{*}{3B/7B Teacher} & FLAN-T5 xlarge & 69.20 & 68.24 & 58.43 & 53.73 & 71.50 & 65.85 & 64.49\\\\\\\\\\\\\\\\\\\\n\\', \\'    & LLaMA 2 & 58.60 & 45.90 & 78.80 & 47.65 & 54.50 & 73.75 & 59.87\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    \\\\n\\', \\'    \\\\\\\\multirow{5}{*}{\\\\\\\\begin{tabular}[c]{@{}c@{}}80M Student\\\\\\\\\\\\\\\\Size: 2.7\\\\\\\\%/1.1\\\\\\\\%\\\\\\\\end{tabular}} & Inference & 16.60 & 19.31 & 20.78 & 13.33 & 38.00 & 47.97 & 26.00 \\\\\\\\\\\\\\\\\\\\n\\', \\'     &  LoRA & 37.80 & 27.12 & 39.93 & 39.80 & 53.75 & 78.05 & 46.08 \\\\\\\\\\\\\\\\\\\\n\\', \\'     &  {Full Fine-tuning} & 41.60 & 27.47 & 42.33 & 42.75 & 56.25 & 78.86 & 48.21\\\\\\\\\\\\\\\\\\\\n\\', \\'     &  {\\\\\\\\ours} & \\\\\\\\textbf{47.60} & \\\\\\\\textbf{31.93} & \\\\\\\\textbf{52.77} & \\\\\\\\textbf{49.22} & \\\\\\\\textbf{62.00} & \\\\\\\\textbf{82.11} & \\\\\\\\textbf{54.27}\\\\\\\\\\\\\\\\\\\\n\\', \\'     &  {$\\\\\\\\Delta_{FF}$} & $\\\\\\\\uparrow 14.42\\\\\\\\%$ & $\\\\\\\\uparrow 16.24\\\\\\\\%$ & $\\\\\\\\uparrow 24.66\\\\\\\\%$ & $\\\\\\\\uparrow 15.13\\\\\\\\%$ & $\\\\\\\\uparrow 10.22\\\\\\\\%$ & $\\\\\\\\uparrow 4.12\\\\\\\\%$ & $\\\\\\\\uparrow 12.57\\\\\\\\%$\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'     \\\\n\\', \\'    \\\\\\\\multirow{5}{*}{\\\\\\\\begin{tabular}[c]{@{}c@{}}250M Student\\\\\\\\\\\\\\\\Size: 8.3\\\\\\\\%/3.6\\\\\\\\%\\\\\\\\end{tabular}} & Inference & 31.00 & 23.00 & 30.47 & 30.78 & 48.00 & 57.72 & 36.83 \\\\\\\\\\\\\\\\\\\\n\\', \\'    &  LoRA & 51.40 & 37.25 & 47.66 & 53.14 & 62.00 & 82.93 & 55.73\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {Full Fine-tuning} & 56.60 & 38.88 & 47.55 & 52.55 & 64.75 & 89.43 & 58.29\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {\\\\\\\\ours} & \\\\\\\\textbf{64.20} & \\\\\\\\textbf{47.98} & \\\\\\\\textbf{60.17} & \\\\\\\\textbf{60.78} & \\\\\\\\textbf{66.25} & \\\\\\\\textbf{90.24} & \\\\\\\\textbf{64.94}\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {$\\\\\\\\Delta_{FF}$} & $\\\\\\\\uparrow 13.43\\\\\\\\%$ & $\\\\\\\\uparrow 23.41\\\\\\\\%$ & $\\\\\\\\uparrow 26.54\\\\\\\\%$ & $\\\\\\\\uparrow 15.66\\\\\\\\%$ & $\\\\\\\\uparrow 2.32\\\\\\\\%$ & $\\\\\\\\uparrow 0.91\\\\\\\\%$ & $\\\\\\\\uparrow 11.40\\\\\\\\%$\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    \\\\n\\', \\'    \\\\\\\\multirow{5}{*}{\\\\\\\\begin{tabular}[c]{@{}c@{}}780M Student\\\\\\\\\\\\\\\\Size: 26.0\\\\\\\\%/11.1\\\\\\\\%\\\\\\\\end{tabular}} & Inference & 50.40 & 51.07  & 51.90 & 39.80 & 64.25 & 63.41 & 53.47\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  LoRA & 64.00 & 57.77 & 57.02 & 68.63 & 70.25 & 86.18 & 67.31\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {Full Fine-tuning} & 71.20 & 62.92 & 58.43 & 68.82 & 70.25 & 90.24 & 70.31\\\\\\\\\\\\\\\\\\\\n\\', \\'    & {\\\\\\\\ours} & \\\\\\\\textbf{74.40} & \\\\\\\\textbf{64.29} & \\\\\\\\textbf{67.90} & \\\\\\\\textbf{70.98} & \\\\\\\\textbf{73.00} & \\\\\\\\textbf{92.68} & \\\\\\\\textbf{73.88}\\\\\\\\\\\\\\\\\\\\n\\', \\'    &  {$\\\\\\\\Delta_{FF}$} & $\\\\\\\\uparrow 4.49\\\\\\\\%$ & $\\\\\\\\uparrow 2.18\\\\\\\\%$ & $\\\\\\\\uparrow 16.21\\\\\\\\%$ & $\\\\\\\\uparrow 3.14\\\\\\\\%$ & $\\\\\\\\uparrow 3.91\\\\\\\\%$ & $\\\\\\\\uparrow 2.70\\\\\\\\%$ & $\\\\\\\\uparrow 5.07\\\\\\\\%$\\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\n\\', \\'    \\\\\\\\bottomrule\\\\n\\', \\'  \\\\\\\\end{tabular}\\\\n\\', \\'\\\\\\\\end{table*}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Learning a Small Student}\\\\n\\', \\'A straightforward strategy to incorporate rationales as supervision is to append each rationale \\\\\\\\(R_i^m\\\\\\\\) generated by the teacher models as supplementary input to the student model, along with the question \\\\\\\\(Q_i\\\\\\\\) and options \\\\\\\\(O_i\\\\\\\\). However, this method faces challenges due to limitations in computational resources at the inference stage, especially because rationales must be pre-generated for every data sample in both training and test sets~\\\\\\\\cite{wang2022pinto}. To overcome this issue, we employ rationales as a form of supervisory signal during the training process to develop a model that is adept at generating its own explanations. Subsequently, this trained model can be utilized on the test set, eliminating the need for pre-generated rationales to facilitate accurate reasoning. Specifically, \\\\\\\\ours integrates rationales from multiple teacher models into a unified multi-task instruction tuning framework. This necessitates the assignment of a unique prefix \\\\\\\\(p\\\\\\\\) for distinguishing between learning tasks from different teachers. The student model is trained not only to predict labels but also to generate rationales akin to those produced by the teachers. Accordingly, the overall loss function $\\\\\\\\mathcal{L}$ is as follows:\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'\\\\\\\\mathcal{L} = \\\\\\\\mathcal{L}_A + \\\\\\\\sum_{m=1}^{M}\\\\\\\\alpha^m\\\\\\\\mathcal{L}_{T^m}, \\\\n\\', \\'\\\\\\\\label{eq:total_loss}\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'where \\\\\\\\(\\\\\\\\mathcal{L}_A\\\\\\\\) denotes the objective of learning from ground truth answers, $\\\\\\\\mathcal{L}_{T^m}$ indicates the objective of learning from $m$-th teacher, $\\\\\\\\alpha^m$ is the importance weight for $T^m$, and $M$ is the number of teacher LLMs. Formally, $\\\\\\\\mathcal{L}_A$ and $\\\\\\\\mathcal{L}_{T^m}$ are defined as follows:\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'\\\\\\\\mathcal{L}_A = \\\\\\\\frac{1}{N} \\\\\\\\sum_{i=1}^{N} \\\\\\\\ell(S(Q_i, O_i, p_A; \\\\\\\\theta_S), A_i),\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'    \\\\\\\\mathcal{L}_{T^m} = \\\\\\\\frac{1}{N} \\\\\\\\sum_{i=1}^{N} \\\\\\\\ell(S(Q_i, O_i, p_m; \\\\\\\\theta_S), R_i^m),\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'where $N$ is the number of data samples, $\\\\\\\\ell$ indicates the cross-entropy loss between the predicted and target tokens. Here $\\\\\\\\mathcal{L}_A$ encourages the student $S$ to generate ground truth answer $A_i$ by minimizing the difference between it and the student output given the question $Q_i$, options $O_i$, and instruction prefix $p_A$ for generating answers. On the other hand, $\\\\\\\\mathcal{L}_T^m$ facilitates the student $S$ to mimic the reasoning capability of teacher $T^m$ by learning from its rationale $R_i^m$, with the guidance of instruction prefix $p_m$ for $T^m$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\section{Experiments}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Experimental Setup}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\textbf{Datasets.} \\\\n\\', \\'We follow the setup in GNP \\\\\\\\cite{gnp} for the usage of commonsense reasoning and biomedical reasoning datasets, including OpenBookQA (OBQA)~\\\\\\\\cite{OpenBookQA2018}, The AI2 Reasoning Challenge (ARC)~\\\\\\\\cite{Clark2018ThinkYH}, Physical Interaction Question Answering (PIQA)~\\\\\\\\cite{Bisk2020PIQA}, RiddleSense (Riddle)~\\\\\\\\cite{lin-etal-2021-riddlesense}, PubMedQA (PQA)~\\\\\\\\cite{jin2019pubmedqa}, and BioASQ~\\\\\\\\cite{Tsatsaronis2015BIOASQ}.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Baselines.} \\\\n\\', \"We benchmark \\\\\\\\ours against the teacher\\'s performance and various training approaches, including an Inference configuration without training, an efficient training method LoRA \\\\\\\\cite{hu2022lora} that updates a subset of parameters, and the full fine-tuning that updates all the parameters in the student.\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Implementation Details.} For \\\\\\\\ours, we set the learning rate to \\\\\\\\(5 \\\\\\\\times 10^{-5}\\\\\\\\), batch size to 16, maximum input length to 1024, and epoch to 1. Trade-off weights \\\\\\\\(\\\\\\\\alpha_{T_n}\\\\\\\\) are explored within \\\\\\\\{0.01, 0.1, 0.5, 1, 2, 3\\\\\\\\}. For the choice of LLMs, we use FLAN-T5 ~\\\\\\\\cite{chung2022scaling} small (80M), base (250M), and large (780M) as the student, and FLAN-T5 xlarge (3B) and LLaMA 2-chat ~\\\\\\\\cite{touvron2023llama2} (7B) as teachers. Experiments are conducted on four NVIDIA Tesla H100 GPUs.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\subsection{Performance Comparison}\\\\n\\', \\'\\\\\\\\textbf{Comparison to Students Learning Methods.}\\\\n\\', \\'The results of six datasets and two reasoning tasks are shown in Table ~\\\\\\\\ref{Overall}. From the table, we observe that the employment of a full-finetuning method, despite its theoretically enhanced capacity for parameter adjustment, does not consistently yield superior results to LoRA. Conversely, \\\\\\\\ours demonstrates substantial performance enhancements across all datasets and LLM sizes. In total, \\\\\\\\ours achieves an average enhancement of \\\\\\\\textbf{+12.57\\\\\\\\%}, \\\\\\\\textbf{+11.40\\\\\\\\%} and \\\\\\\\textbf{+5.07\\\\\\\\%} for students with 80M, 250M, and 780M parameters, respectively. This validates the effectiveness of \\\\\\\\ours and underscores the importance and benefits of learning from teachers.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\noindent \\\\\\\\textbf{Comparison to Teachers.} \\\\n\\', \\'\\\\\\\\ours also shows superior performance compared to the teacher models. For example, a 780M student can achieve an average performance of 73.88 across different datasets, which is \\\\\\\\textbf{+14.56\\\\\\\\%} better than the 3B teacher and \\\\\\\\textbf{+23.40\\\\\\\\%} better than the 7B teacher. Moreover, an even smaller student model with 250M parameters can outperform the teachers (\\\\\\\\textbf{+0.70\\\\\\\\%} to 3B, \\\\\\\\textbf{+16.82\\\\\\\\%} to 7B) while using only \\\\\\\\textbf{8.3\\\\\\\\%} and \\\\\\\\textbf{3.6\\\\\\\\%} of the teacher parameters.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\subsection{Ablation Study}\\\\n\\', \\'For a comprehensive evaluation, we conduct ablation studies to validate the contribution of the in-context example generator and rationales from multiple teachers. To facilitate this, we create three ablation variants of \\\\\\\\ours. \\\\n\\', \\'\\\\\\\\begin{itemize}[leftmargin=*]\\\\n\\', \\'    \\\\\\\\item \\\\\\\\textbf{w/o in-context} rules out the in-context examples during rationale generation.\\\\n\\', \\'    \\\\\\\\item \\\\\\\\textbf{w/o LLaMa} and \\\\\\\\textbf{w/o T5} exclude the rationale supervision from the corresponding teacher model during distillation.\\\\n\\', \\'\\\\\\\\end{itemize}\\\\n\\', \\'Table ~\\\\\\\\ref{tab:results} depicts the comparison between the full \\\\\\\\ours model and its ablations on the six datasets. According to the table, the decreasing performances of removing the in-context examples and teacher rationales demonstrate their effectiveness in enhancing the model. By combining all the designs, the proposed \\\\\\\\ours achieves the best performance. These findings indicate that learning high-quality rationale signals from multiple teachers is beneficial in distilling a small LLM with better reasoning ability.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{table}[t]\\\\n\\', \\'  \\\\\\\\caption{Impact of in-context examples and contributions of various teachers.}\\\\n\\', \\'  \\\\\\\\vspace{-0.1in}\\\\n\\', \\'  \\\\\\\\label{tab:results}\\\\n\\', \\'  \\\\\\\\resizebox{\\\\\\\\linewidth}{!}{\\\\n\\', \\'  \\\\\\\\begin{tabular}{ccccccc}\\\\n\\', \\'    \\\\\\\\toprule\\\\n\\', \\'    &  \\\\\\\\multicolumn{4}{c}{Commonsense} & \\\\\\\\multicolumn{2}{c}{Biomedical} \\\\\\\\\\\\\\\\ \\\\n\\', \\'    \\\\\\\\cmidrule{2-7}\\\\n\\', \\'    \\\\\\\\multirow{-2.3}{*}{\\\\\\\\textbf{Variant}} & {\\\\\\\\textbf{OBQA}} & {\\\\\\\\textbf{ARC}} & {\\\\\\\\textbf{PIQA}} & {\\\\\\\\textbf{Riddle}} & {\\\\\\\\textbf{PQA}} & {\\\\\\\\textbf{BioASQ}} \\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\midrule\\\\n\\', \\'    {w/o in-context}  & 73.20 & 63.09 & 66.27 & 69.22 & 70.75 & 86.99 \\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\n\\', \\'    {w/o LLaMA}  & 73.00 & 62.32 & 66.70 & 68.82 & 69.25 & 87.81 \\\\\\\\\\\\\\\\\\\\n\\', \\'\\\\n\\', \\'    {w/o T5}  & 73.80 & 61.80 & 66.49 & 68.63 & 69.50 & 88.62 \\\\\\\\\\\\\\\\\\\\n\\', \\'\\\\n\\', \\'    {\\\\\\\\ours} & \\\\\\\\textbf{74.40} & \\\\\\\\textbf{64.29} & \\\\\\\\textbf{67.90} & \\\\\\\\textbf{70.98} & \\\\\\\\textbf{73.00} & \\\\\\\\textbf{92.68} \\\\\\\\\\\\\\\\\\\\n\\', \\'    \\\\\\\\bottomrule\\\\n\\', \\'  \\\\\\\\end{tabular}\\\\n\\', \\'  }\\\\n\\', \\'  \\\\\\\\vspace{-0.1in}\\\\n\\', \\'\\\\\\\\end{table}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\subsection{Parameter Sensitivity}\\\\n\\', \\'\\\\n\\', \"The exploration of trade-off weights reveals the model\\'s adaptability across different choices of parameter values. We conduct sensitivity experiments on datasets ARC for commonsense reasoning and PQA for biomedical reasoning in ~\\\\\\\\ref{fig:sensitivity}. According to the figure, we observe that the optimal parameters for various datasets and tasks differ. The reason for this phenomenon is that biomedical reasoning questions are often lengthy and complex, weakening the impact of rationales from teachers and making a small value of $\\\\\\\\alpha$ sufficient. In contrast, commonsense reasoning questions are typically concise and straightforward, making the rationales from teacher models valuable and leading to a large value of $\\\\\\\\alpha$.\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{figure}[t]\\\\n\\', \\'  \\\\\\\\centering\\\\n\\', \\'  \\\\\\\\includegraphics[width=\\\\\\\\columnwidth]{Figures/param_sen.png}\\\\n\\', \\'  \\\\\\\\vspace{-0.25in}\\\\n\\', \\'  \\\\\\\\caption{Performance w.r.t. different values of weight $\\\\\\\\alpha$.}\\\\n\\', \\'  \\\\\\\\vspace{-0.15in}\\\\n\\', \\'  \\\\\\\\label{fig:sensitivity}\\\\n\\', \\'\\\\\\\\end{figure}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\vspace{-0.05in}\\\\n\\', \\'\\\\\\\\section{Conclusion}\\\\n\\', \\'In this paper, we propose \\\\\\\\ours, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. \\\\\\\\ours involves several innovative designs, such as inheriting a broader range of knowledge across different teachers and learning contextually appropriate and accurate rationales using an in-context example generator along with a teacher-forcing Chain-of-Thought strategy. Extensive experiments across six datasets and two reasoning tasks validate the superiority of \\\\\\\\ours.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\bibliographystyle{acm}\\\\n\\', \\'\\\\\\\\bibliography{reference}\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\end{document}\\\\n\\']\\n      Analyse paper by section and strictly follow this template but json format:\\n      \\n    {Section Name:  Introduction,\\n    Slide Information: {\\n      Number of Slides: 3,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Introduction,\\n      Image: Figure path if it is present in Introduction section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n    {Section Name:  Method,\\n    Slide Information: {\\n      Number of Slides: 2,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Method,\\n      Image: Figure path if it is present in Method section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n    {Section Name:  Experiments,\\n    Slide Information: {\\n      Number of Slides: 1,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Experiments,\\n      Image: Figure path if it is present in Experiments section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n    {Section Name:  Conclusion,\\n    Slide Information: {\\n      Number of Slides: 1,\\n      Speaker Notes: {\"Slide {Number}\": 150 words speech for slide},\\n      Table: Latex Table to header and row dictionary if present in Conclusion,\\n      Image: Figure path if it is present in Conclusion section (possible selections [\\'Figures/pipeline.png\\', \\'Figures/param_sen.png\\']),\\n      Generative Prompt: Propose a 10 words prompt for simple slide background\\n    }\\n    }\\n    \\n      '}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "x3OkMeJ1jwOZ"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key  = ''\n",
        "\n",
        "def get_completion_from_messages(messages,\n",
        "                                 model=\"gpt-4-turbo-preview\",\n",
        "                                 response_format={ \"type\": \"json_object\" },\n",
        "                                 temperature=0, max_tokens=1200):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=get_completion_from_messages(messages=messages)"
      ],
      "metadata": {
        "id": "VnKJ8ONlkB8B"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def LLM_Output_to_dict(LLM_Output,Content_Num):\n",
        "    Section_Json = []\n",
        "    for x in LLM_Output.split('\\n\\n'):\n",
        "        Section_Json.append(x.replace('```','').lstrip().rstrip().lstrip('json'))\n",
        "    Section_Json = [x for x in Section_Json if not x == '']\n",
        "    result = json.loads(f'{Section_Json[Content_Num]}')\n",
        "    Section_Name = result['Section Name']\n",
        "    Number_of_Slides = result['Slide Information']['Number of Slides']\n",
        "    Slide_Info = [x for x in result['Slide Information']['Speaker Notes'].values()]\n",
        "    Table = result['Slide Information']['Table']\n",
        "    Image = result['Slide Information']['Image']\n",
        "    Generative_Prompt = result['Slide Information']['Generative Prompt']\n",
        "    return Section_Name,Number_of_Slides,Slide_Info,Table,Image,Generative_Prompt\n",
        "\n",
        "\n",
        "\n",
        "Section_Name,Number_of_Slides,Speaker_Notes,Table,Image,Generative_Prompt = LLM_Output_to_dict(output,0)\n",
        "print(Section_Name)\n",
        "print(Number_of_Slides)\n",
        "print(Speaker_Notes)\n",
        "print(Table)\n",
        "print(Image)\n",
        "print(Generative_Prompt)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq55JgJHhRDP",
        "outputId": "c7993506-d84a-4a1d-c404-54d41a85676b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduction\n",
            "3\n",
            "['Today, we embark on an exploration of a groundbreaking approach in the realm of language models, specifically focusing on the transition of knowledge from large to small language models. The motivation behind this research stems from the desire to harness the power of large language models in a more efficient and cost-effective manner. By transferring capabilities from larger models to smaller ones, we aim to achieve high performance without the associated computational overhead.', 'However, the journey is not without its challenges. Existing methods of knowledge distillation, while effective to a degree, face significant limitations in terms of knowledge diversity and the richness of contextual information. This presentation introduces TinyLLM, a novel paradigm designed to overcome these hurdles by leveraging multiple large language models as teachers, thereby enriching the student model with diverse knowledge and deep contextual insights.', \"The essence of TinyLLM lies in its innovative approach to knowledge distillation, which not only focuses on the correct answers but also delves into the rationales behind these answers. By understanding the 'why' and 'how', the student model gains a deeper comprehension of the subject matter, leading to improved reasoning capabilities and performance across various datasets and tasks.\"]\n",
            "None\n",
            "Figures/pipeline.png\n",
            "Innovative knowledge distillation paradigm visualization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bullet_points(prompt):\n",
        "  bullets = []\n",
        "  for info in prompt:\n",
        "    output = get_completion_from_messages(messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are an expert presentation designer that create bullet points from speech\",\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f'''Given the document: {info}\n",
        "          Give me 4 concise, 10 words bullet points for the slide''',\n",
        "        }\n",
        "      ],model='gpt-3.5-turbo',)\n",
        "    bullets.append([x.lstrip('* ').rstrip() for x in output.split('\\n')])\n",
        "  return bullets\n",
        "for x in bullet_points(Speaker_Notes):\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "236j5_156Sn0",
        "outputId": "02044981-d6d6-4913-8deb-4fedbfbe9282"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1. Transitioning knowledge from large to small language models efficiently.', '2. Harnessing power of large models in a cost-effective manner.', '3. Achieving high performance without excessive computational overhead.', '4. Exploring groundbreaking approach in language model transition for efficiency.']\n",
            "['- Challenges in existing knowledge distillation methods: diversity and contextual limitations.', '- Introducing TinyLLM: innovative paradigm using multiple large language models.', '- TinyLLM enriches student model with diverse knowledge and deep insights.', '- Overcoming hurdles by leveraging large language models as teachers.']\n",
            "[\"- TinyLLM: Innovative knowledge distillation approach emphasizing 'why' and 'how'.\", '- Deeper comprehension leads to improved reasoning and performance.', '- Focus on rationales behind correct answers enhances student model understanding.', '- Enhanced reasoning capabilities across datasets and tasks with TinyLLM.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stable Diffusion"
      ],
      "metadata": {
        "id": "24Snr_5Q6HGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stability-sdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AfdGXRA71nc",
        "outputId": "6e28d266-8f3d-4c57-fa87-0ae0ed58f147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stability-sdk\n",
            "  Downloading stability_sdk-0.8.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from stability-sdk) (9.4.0)\n",
            "Collecting grpcio==1.53.0 (from stability-sdk)\n",
            "  Downloading grpcio-1.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-tools==1.53.0 (from stability-sdk)\n",
            "  Downloading grpcio_tools-1.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv (from stability-sdk)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from stability-sdk) (2.0.2)\n",
            "Collecting protobuf==4.21.12 (from stability-sdk)\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools==1.53.0->stability-sdk) (67.7.2)\n",
            "Installing collected packages: python-dotenv, protobuf, grpcio, grpcio-tools, stability-sdk\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.62.0\n",
            "    Uninstalling grpcio-1.62.0:\n",
            "      Successfully uninstalled grpcio-1.62.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed grpcio-1.53.0 grpcio-tools-1.53.0 protobuf-4.21.12 python-dotenv-1.0.1 stability-sdk-0.8.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import warnings\n",
        "from PIL import Image\n",
        "from stability_sdk import client\n",
        "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation"
      ],
      "metadata": {
        "id": "hlJ4pz7i8RDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stability_api = client.StabilityInference(\n",
        "    key=os.environ['STABILITY_KEY'], # API Key reference.\n",
        "    verbose=True, # Print debug messages.\n",
        "    engine=\"stable-diffusion-xl-1024-v1-0\", # Set the engine to use for generation.\n",
        "    # Check out the following link for a list of available engines: https://platform.stability.ai/docs/features/api-parameters#engine\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhTV_sM28E1h",
        "outputId": "970cbbda-8b28-456f-e198-3c75af34811a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stability_sdk.client:Opening channel to grpc.stability.ai:443\n",
            "INFO:stability_sdk.client:Channel opened to grpc.stability.ai:443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers = stability_api.generate(\n",
        "    prompt= Generative_Prompt,\n",
        "    seed=4253978046, # If a seed is provided, the resulting generated image will be deterministic.\n",
        "                     # What this means is that as long as all generation parameters remain the same, you can always recall the same image simply by generating it again.\n",
        "                     # Note: This isn't quite the case for Clip Guided generations, which we'll tackle in a future example notebook.\n",
        "    steps=50, # Amount of inference steps performed on image generation. Defaults to 30.\n",
        "    cfg_scale=8.0, # Influences how strongly your generation is guided to match your prompt.\n",
        "                   # Setting this value higher increases the strength in which it tries to match your prompt.\n",
        "                   # Defaults to 7.0 if not specified.\n",
        "    width=1024, # Generation width, defaults to 512 if not included.\n",
        "    height=1024, # Generation height, defaults to 512 if not included.\n",
        "    samples=1, # Number of images to generate, defaults to 1 if not included.\n",
        "    sampler=generation.SAMPLER_K_DPMPP_2M # Choose which sampler we want to denoise our generation with.\n",
        "                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n",
        "                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m, k_dpmpp_sde)\n",
        ")"
      ],
      "metadata": {
        "id": "gEmXTl9d8Zv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for resp in answers:\n",
        "    for artifact in resp.artifacts:\n",
        "        if artifact.finish_reason == generation.FILTER:\n",
        "            warnings.warn(\n",
        "                \"Your request activated the API's safety filters and could not be processed.\"\n",
        "                \"Please modify the prompt and try again.\")\n",
        "        if artifact.type == generation.ARTIFACT_IMAGE:\n",
        "            img = Image.open(io.BytesIO(artifact.binary))\n",
        "            img.save(str(artifact.seed)+ \".png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX1T_M7H8kDy",
        "outputId": "bcd1796d-f3ce-4d30-c24a-bca940748edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stability_sdk.client:Sending request.\n",
            "INFO:stability_sdk.client:Got answer  with artifact types ['ARTIFACT_IMAGE'] in 8.80s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Powerpoint Pipeline"
      ],
      "metadata": {
        "id": "iivVc8nC6Iu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-pptx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qgQo7Ia8WpL",
        "outputId": "052dadfc-e16d-42ce-f91d-718e9deb3473"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/471.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/471.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.0 python-pptx-0.6.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Section_Name = 'Introduction'\n",
        "Number_of_Slides = 3\n",
        "Speaker_Notes = ['Today, we embark on an exploration of a groundbreaking approach in the realm of language models, specifically focusing on the transition of knowledge from large to small language models. The motivation behind this research stems from the desire to harness the power of large language models in a more efficient and cost-effective manner. By transferring capabilities from larger models to smaller ones, we aim to achieve high performance without the associated computational overhead.', 'However, the journey is not without its challenges. Existing methods of knowledge distillation, while effective to a degree, face significant limitations in terms of knowledge diversity and the richness of contextual information. This presentation introduces TinyLLM, a novel paradigm designed to overcome these hurdles by leveraging multiple large language models as teachers, thereby enriching the student model with diverse knowledge and deep contextual insights.', \"The essence of TinyLLM lies in its innovative approach to knowledge distillation, which not only focuses on the correct answers but also delves into the rationales behind these answers. By understanding the 'why' and 'how', the student model gains a deeper comprehension of the subject matter, leading to improved reasoning capabilities and performance across various datasets and tasks.\"]\n",
        "Table = None\n",
        "Image = 'Figures/pipeline.png'\n",
        "Generative_Prompt = 'Innovative knowledge distillation paradigm visualization'"
      ],
      "metadata": {
        "id": "HxNTtvlPBwrQ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Inches\n",
        "\n",
        "\n",
        "class PresentationCreator:\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.prs = Presentation()\n",
        "        # Define slide layout mappings\n",
        "        self.layout_mappings = {\n",
        "            \"1\": self.prs.slide_layouts[0],  # Title Slide\n",
        "            \"2\": self.prs.slide_layouts[1],  # Title and Content\n",
        "            \"9\": self.prs.slide_layouts[8],  # Picture with Caption\n",
        "            # Add more mappings as needed for other layouts\n",
        "        }\n",
        "\n",
        "    def add_slide(self, slide_info: dict) -> None:\n",
        "        \"\"\"Add slide to presentation.\"\"\"\n",
        "        # Extract slide info\n",
        "        layout_index = slide_info.get(\"layout\", \"2\")\n",
        "        if layout_index == \"1\":\n",
        "            self.add_title_layout(slide_info)\n",
        "        elif layout_index == \"2\":\n",
        "            self.add_title_content_layout(slide_info)\n",
        "        elif layout_index == \"9\":\n",
        "            self.add_picture_caption_layout(slide_info)\n",
        "\n",
        "    def add_title_layout(self, slide_info: dict) -> None:\n",
        "        \"\"\"Add slide with title layout.\"\"\"\n",
        "        slide = self.prs.slides.add_slide(self.prs.slide_layouts[0])\n",
        "        title_shape = slide.shapes.title\n",
        "        title_shape.text = slide_info[\"title\"]\n",
        "\n",
        "    def add_title_content_layout(self, slide_info: dict) -> None:\n",
        "        \"\"\"Add slide with title and content layout.\"\"\"\n",
        "        slide = self.prs.slides.add_slide(self.prs.slide_layouts[1])\n",
        "        title_shape = slide.shapes.title\n",
        "        title_shape.text = slide_info[\"title\"]\n",
        "        content_shape = slide.placeholders[1]\n",
        "        content_frame = content_shape.text_frame\n",
        "        content_frame.text = slide_info[\"content\"]\n",
        "\n",
        "    def add_picture_caption_layout(self, slide_info: dict) -> None:\n",
        "        \"\"\"Add slide with picture and caption layout.\"\"\"\n",
        "        slide = self.prs.slides.add_slide(self.prs.slide_layouts[8])\n",
        "        title_shape = slide.shapes.title\n",
        "        title_shape.text = slide_info[\"title\"]\n",
        "        # title_shape.left = Inches(2)\n",
        "        # title_shape.text_frame.paragraphs[0].font.size = Inches(0.25)\n",
        "        left = Inches(1)  # Adjust left position as needed\n",
        "        top = Inches(1.5)  # Adjust top position as needed\n",
        "        width = Inches(5)  # Adjust width as needed\n",
        "        height = Inches(3)  # Adjust height as needed\n",
        "        slide.shapes.add_picture(slide_info[\"image\"], left, top, width, height)\n",
        "\n",
        "    def save_presentation(self, file_name: str) -> None:\n",
        "        \"\"\"Save presentation to file.\"\"\"\n",
        "        self.prs.save(file_name)\n",
        "\n",
        "    def convert_presentation_to_image(self, file_name: str) -> None:\n",
        "        \"\"\"Convert presentation to image.\"\"\"\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "_x1tn9tc6H62"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "No53PB4l8YUX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Presentation_Object = PresentationCreator()\n",
        "Layout = '2'\n",
        "bullets = bullet_points(Speaker_Notes)\n",
        "Slide_info = {'layout':Layout,'title':Section_Name,'content':'\\n'.join(bullets[0]),'image':Image}\n",
        "print(Slide_info)\n",
        "Presentation_Object.add_slide(Slide_info)\n",
        "Presentation_Object.add_picture_caption_layout(Slide_info)"
      ],
      "metadata": {
        "id": "JdlSIDmJ6K8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134b53d0-db7d-467e-afc5-646afa280ee0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'layout': '2', 'title': 'Introduction', 'content': '1. Transitioning knowledge from large to small language models efficiently.\\n2. Harnessing power of large models in cost-effective manner.\\n3. Achieving high performance without computational overhead through knowledge transfer.\\n4. Exploring groundbreaking approach in language models for efficiency and effectiveness.', 'image': 'Figures/pipeline.png'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Presentation_Object.save_presentation('test.pptx')"
      ],
      "metadata": {
        "id": "IBaUe88T_l05"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3EKQF-5_po3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}